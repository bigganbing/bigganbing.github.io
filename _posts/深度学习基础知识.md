---
typora-root-url: ..
---

### 深度学习基础知识

#### 一、激活函数 （activation function） 

##### 1. 为什么需要激活函数？

①无论多少个线性层直接堆积在一起（即输入经过很多次线性变换）， 输出都是输入的线性组合 ，完全能够被一个线性层所替代；

②激活函数是为了给模型增加非线性因素，因为并非所有的数据都是线性可分的，添加了激活函数的线性层能够拟合任何函数。

- 例如：以下两次线性变换本质上可通过一次线性变换完成

<div align="center"><img src="/img/v2-7c6e12aed30bf315eed8df6476d7ef7b_r-1572695574897.jpg" alt="preview" style="zoom:80%;" /></div>
​       上面公式合并同类项后等价于下面公式（两次线性变换==一次线性变换）： 
$$
y=x_{1}\left(w_{2-1} w_{1-11}+w_{2-2} w_{1-12}+w_{2-3} w_{1-13}\right)+x_{2}\left(w_{2-1} w_{1-21}+w_{2-2} w_{1-22}+w_{2-3} w_{1-23}+w_{2-2} b_{1-2}+w_{2-3} b_{1-3}\right.
$$

- 例如：以下数据就不是线性可分的，而需要一个非线性函数才能将红色和蓝色数据区分开。 

<div align='center'><img src="/img/v2-10f91a9a61f22aea9a46ebaf63c1e3ad_b-1572696348729.jpg" alt="img" style="zoom:100%;" /></div>


#####   2. 常用激活函数

![1_6HURswvEIPSCYrAd1U7RUw](/img/1_6HURswvEIPSCYrAd1U7RUw.png)

- Sigmoid函数将输出映射到**0~1**之间，即函数的输出**不是零中心**的 ，且存在**梯度消失**的问题。
  $$
  \text{Sigmoid}(x) = \frac{1}{1 + e^{-x}}
  $$

- Tanh双曲正切函数将输出映射到**-1~1**之间，相比Sigmoid函数的有点在于输出是**零中心**的，较Sigmoid收敛更快，但也存在**梯度消失**的问题。
  $$
  \text{Tanh}(x) =  \frac{e^x - e^{-x}} {e^x + e^{-x}}
  $$
  

- ReLU函数的输出**不是零中心**的、也**不是有界的**，因为输入小于0则梯度为0，可能存在某些神经元得不到更新的问题（ **Dead ReLU Problem** ）， 它前向和求导都不含指数运算，运算较快，而且收敛速度也较前两者快。
  $$
  \text{ReLU}(x)= \max(0, x)
  $$

- LeakyReLU解决了ReLU的 **Dead ReLU Problem**
  $$
  \text{LeakyReLU}(x)= \max(αx, x)
  $$
  $其中α是一个很小的常数，一般取0.01$

**分析**：

Sigmoid和Tanh网络能够学习一段连续的**平滑函数**，而 ReLU和 LeakyReLU网络学习的是**分段函数**，以**分段线性**方式分离数据。

**我的疑问：**

为什么ReLU收敛比Sigmoid、Tanh更快？

为什么零中心也会影响收敛？Sigmoid的输出为正， 后一层的神经元以此为输入，会使得后一层的梯度都为正，进而影响收敛。这种说法怎么理解？



##### 3. 激活函数应具备特性

-  非线性：即导数不是常数。这个条件是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。 
-  几乎处处可微：可微性保证了在优化中梯度的可计算性。 
-  计算简单 ： 激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。 
-  非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。 
-  单调性（monotonic）：即导数符号不变。 单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。
-  输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定。
-  接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。 



#### 二、梯度消失和梯度爆炸的问题

##### 1.什么是梯度消失、梯度爆炸

梯度消失和梯度爆炸一般发生在神经网络中较前的网络层，这是由反向传播算法导致的，即复合函数求导链式法则的多项连乘引起。

梯度消失因为前面层的梯度接近0，会使得前面层的网络参数无法得到更新，即停止了学习。

梯度爆炸因为前面层的梯度很大，会使得前面层的网络参数变化很大，进而导致网络不稳定，很难学习到一个较好的结果，甚至可能出现参数值为NaN的情况。

##### 2.梯度消失和梯度爆炸原因

(1) 以一个每层只有一个神经元的4层网络为例进行分析：

<div align="center"><img src="/img/TIM截图20191103135712.jpg" alt="TIM截图20191103135712" style="zoom:80%;" /></div>

从上面$w_0$的梯度计算式我们知道，决定梯度大小的**影响因素**有：

- **网络深度**，*影响连乘的项数*

- **初始化参数和输入**，*决定部分项的值*
- **激活函数**，*决定部分项的值*

(2) 现从网络深度以及激活函数角度进行解释： 

- 网络深度

   在网络很深时，若各层权重初始化较小，且激活函数的导数也较小。那么很多个绝对值小于1的数连乘后，结果数值就会接近于0，导致**梯度消失**。 

   若权重初始化较大，大到乘以激活函数的导数都大于1，那么很多个绝对值大于1的数连乘后，结果数值就会很大，导致**梯度爆炸**。 

- 激活函数

   如果激活函数选择不合适，比如使用 Sigmoid，梯度消失就会很明显了。因为Sigmoid的导函数在两侧都接近0，而且最大值为0.25。

##### 3.解决梯度消失方法

- 对于循环神经网络，使用LSTM、GRU替换LSTM。
- 残差连接， 深层的网络梯度通过跳级连接路径直接返回到浅层部分，使得网络无论多深都能将梯度进行有效的回传。 
- 使用RelU、leakReLu激活函数替换Sigmoid、Tanh。
-  Batchnorm ，通过规范化操作将输出信号x规范化到均值为0，方差为1， 保证网络的稳定性 ，并加速网络收敛速度 。

##### 4.解决梯度爆炸方法

-  Batchnorm ，通过规范化操作将输出信号x规范化到均值为0，方差为1， 保证网络的稳定性 ，并加速网络收敛速度 。
-  梯度剪切， 设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。
-  权重正则化，比较常见的是L1和L2正则 。






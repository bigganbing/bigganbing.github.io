---
typora-root-url: ..
---

## 文本分类

### 一、文本分类问题介绍

#### 1.什么是文本分类

 对给定的文档，将其分类为$n$个类别中的一个或多个 。

#### 2.文本分类应用场景

情感分析、文章主题分析、垃圾邮件识别、黄反违禁识别等等。

#### 3.文本分类问题类型

**Note：**不同类型的问题吧，处理方法和应用的模型可能不同。

- **文本长度不同**

  长文本分类、短文本分类

- **文本语言不同**

  中文文本分类、英文文本分类、混合语言文本分类、等等

- **标签类别个数不同**

  二分类、多分类、多标签分类

#### 4.文本分类问题实例

[ 英文垃圾信息分类 ](https://god.yanxishe.com/11)

[互联网新闻情感分析](https://datafountain.cn/competitions/350)

[汽车行业用户观点主题及情感识别]( https://datafountain.cn/competitions/310 )

[文本点击率预估](https://www.kesci.com/home/competition/5cc51043f71088002c5b8840/content/1)，此问题不完全属于文本分类问题，但是处理手法类似。



### 二、文本分类方法概述

#### 1、文本分类一般步骤

<div align="center"><img src="/img/553842-20170429175935944-700874016-1572003214178.png" alt="这里写图片描述" /></div>
 **特征工程**：提取文本的特征，并将其转化成计算机可理解的格式。

**分类器**：对特征工程得到的文本表示进行分类，即将一个文本向量映射到具体的类别。
$$
X[x_1,x_2,x_3,x_4,...,x_n] \rightarrow y \in Y[y_1,y_2,...y_m]
$$

#### 2、文本预处理

一般在文本表示之前，为了能更有效的提取文本的特征，会对文本进行预处理。

- 中文

  1. **分词**

     将一段文本切分成一个的单词列表。

     分词原因：很多研究表明特征粒度为词粒度远好于字粒度 ，词蕴含更多的语义信息。

     分词工具：$Jieba$, $SnowNLP$, $PkuSeg$, $THULAC$, $HanLP$等等。

     示例：

     ```python
     import jieba
     text = '一如既往的好吃，尤其是那个翡翠面条'
     print(list(jieba.cut(text)))
     # ['一如既往', '的', '好吃', '，', '尤其', '是', '那个', '翡翠', '面条']
     ```

  2. **去停用词**

     去掉像“然而”、 标点符号等只能反映句子语法结构的词语，而不能反映文献的主题的词汇。

- 英文

  分词，大小写转换，还原缩写，去停用词，词形还原等等。

  Note：英文单词之间有天然的空格，能够直接基于空格分词。

  ```python
  import nltk
  from nltk.corpus import stopwords
  import re
  
  text = 'The food smells so bad that I can\'t swallow it'
  
  # 转化为小写
  text =text.lower()
  
  # 还原缩写
  text = re.sub(r"can\'t", "can not", text)
  print(text) 	
  # the food smells so bad that i can not swallow it
  
  # 分词
  text = text.split()
  print(text)	
  # ['the', 'food', 'smells', 'so', 'bad', 'that', 'i', 'can', 'not', 'swallow', 'it']
  
  # 去停用词
  text = [w for w in text if w not in stopwords.words('english')]
  print(text)
  # ['food', 'smells', 'bad', 'swallow']
  
  # 词形还原
  poter = nltk.WordNetLemmatizer()
  text = [poter.lemmatize(w) for w in text]
  print(text)
  # ['food', 'smell', 'bad', 'swallow']
  ```

**Note**：以上提到的文本预处理方法并不全面，并非所有任务的都需要使用这些方法，也并非使用了效果就更好。而是应该结合所使用的模型和方法，结合文本自身的特点有选择的使用。

#### 3、文本分类方法简介

##### (1) 传统机器学习方法 

**表示**：使用词袋模型、TI-IDF模型等方式，将不同的待分类的文本表示成相同长度的向量。

**分类**：使用 支持向量机、决策树、 最近邻算法 、朴素贝叶斯等分类算法对提取的文本向量表示进行训练和分类。

##### (2) 深度学习方法 

**表示**：基于词向量，使用神经网络，如RNN（循环神经网络）、CNN（卷积神经网络）等结构，将不同的待分类的文本表示成相同长度的向量。

**分类**：

- 二分类（Sigmoid +BCELoss）
  
  **训练时**，在网络的输出层，先将通过前一部分神经网络得到的文本向量映射成单值，然后使用Sigmoid函数，将其映射到0,1之间，最后用二分类的交叉熵损失函数，计算模型的损失，然后反向传播，计算梯度并更新参数。
  

**预测时**，经过Sigmoid函数得到的 单值可以作为当前文本为正例，即类别为1的概率，当其大于$\frac{1}{2}$，可认为该文本为类别1，否则为类别0。
$$
  x = W^TX,其中X=[x_1,x_2,...x_n]^T,W=[w_1,w_2,...w_n]^T
$$

$$
  \text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}
$$

$$
  loss = - \left[ t \cdot \log \sigma(x)
          + (1 - t) \cdot \log (1 - \sigma(x)) \right]
$$

  $ \quad 其中，\sigma(x)表示Sigmoid(x)$，$t为文本实际类别。$

  $\quad Sigmoid函数图像如下：$

  <div align="center"><img src="/img/Dy-vlK5XcAAvE5U.jpg" alt="Image result for sigmoid" align="center" style="zoom:80%;" /></div>
- 多分类（Softmax+CrossEntropyLoss）
  
  **训练时**，在网络的输出层，先将通过前一部分神经网络得到的文本向量映射成“**长度=类别数目**”的向量，然后使用$Softmax$函数归一化，将向量中每一个元素映射到0,1之间，最后用使用交叉熵损失函数，计算模型的损失，然后反向传播，计算梯度并更新参数。
  

**预测时**，经过$Softmax$函数归一化得到“**长度=类别数目**”的向量，这个向量的每一个元素可以看作该文本被分类为相应类别的概率，可将概率最大的类别作为当前文本的类别。
$$
  x=W^TX, 其中X\in R^{n×1},W\in R^{n×m}, x=[x_1,x_2,...,x_m]^T \in R^{m×1}
$$

$$
\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
$$

$$
  \text{loss}(x, c) = -\log\left(Softmax(x_c)\right)= -\log\left(\frac{\exp(x[c])}{\sum_j \exp(x[j])}\right)= -x[c] + \log\left(\sum_j \exp(x[j])\right)
$$

  $ \quad 其中，c为样本实际类别。$

  $数值计算过程如下：$

  <div align='center'><img src="/img/TIM图片20191026112311.png" alt="TIM图片20191026112311" style="zoom:70%;" /></div>
- 多标签分类

  多标签问题，可以转化为在每个标签上的二分类问题，对每个标签使用Sigmoid 和BCELoss，然后对各个标签的损失取平均值作为样本的损失。 

  $$
  loss = - \frac {\sum_i^m \left[ t_i \cdot \log \sigma(x_i)
          + (1 - t_i) \cdot \log (1 - \sigma(x_i)) \right]}{m}
  $$
  



### 三、传统机器学习方法进行特征提取

#### 1、词袋模型 (Bag Of Words) 

##### (1) 最朴素词袋模型

不考虑文本中各词出现的顺序，将文本表示成**词表长度**的向量，当**词表**中某词在文本中出现，则向量对应位置的元素值为1，否则为0。

示例：

假如训练集只包含三个句子，"it is cold"，"is it cold"，"is it cold is not"。（假设这里的句子都已经经过大小写、词形还原的预处理）

那么，训练集中所有出现的单词可构成**词表**如下：

|  0   |  1   |  2   |  3   |
| :--: | :--: | :--: | :--: |
|  it  |  is  | cold | not  |

①"it is cold"可表示成：$X_1 = [1,1,1,0]$

②"is it cold"可表示成：$X_2=[1,1,1,0]$

③"is it cold is not"可表示成：$X_3=[1,1,1,1]$

##### (2) 考虑词频的词袋模型

向量中某元素的值为，词表中对应词在文本中出现的次数或词频。则

①"it is cold"可表示成：$X_1 = [1,1,1,0]$或$[\frac{1}{3},\frac{1}{3},\frac{1}{3},0]$

②"is it cold"可表示成：$X_2=[1,1,1,0]$或$[\frac{1}{3},\frac{1}{3},\frac{1}{3},0]$

③"is it cold is not"可表示成：$X_3=[1,2,1,1]$或$[\frac{1}{5},\frac{2}{5},\frac{1}{5},\frac{1}{5}]$

##### (3) 考虑词组（N-gram）的词袋模型

不仅考虑词出现的次数，还考虑N元组出现的次数。

**以N=2为例**

词表不仅包含所有的单词，还应包含所有的二元组，此时文本向量长度也相应增加。

-"it is cold"包含的二元组有"it is"，"is cold"；

-"is it cold"包含的二元组有"is it"，"it cold"；

-"is it cold is not"包含的二元组有"is it"，"it cold"，"cold is"，"is not"。

此时，训练集中所有的单词和二元组构成词表如下：

|  0   |  1   |  2   |  3   |   4   |    5    | 6     |    7    |    8    |   9    |
| :--: | :--: | :--: | :--: | :---: | :-----: | ----- | :-----: | :-----: | :----: |
|  it  |  is  | cold | not  | it is | is cold | is it | it cold | cold is | is not |

①"it is cold"可表示成：$X_1 = [1,1,1,0,1,1,0,0,0,0]$

②"is it cold"可表示成：$X_2=[1,1,1,0,0,0,1,1,0,0]$

③"is it cold is not"可表示成：$X_3=[1,1,1,1,0,0,1,1,1,1]$

#### 2、TF-IDF模型——考虑权重的词袋模型

同前面的词袋模型，将一个文本表示词表长度的向量，不同的是，这个向量的每一个元素的不再是表示单词是否在文本中出现的0或1，也不是单词在文本中出现的次数或词频，而是每一个单词的$tf$$idf$值。
$$
tfidf =tf×idf
$$

- tf 称为词频(term frequency)，即，某个词在文档中的出现频率。用于计算该词描述文档内容的能力。 
  $$
  tf=\frac{某个词在文本中出现的次数}{文本的单词总数}
  $$

- Idf称为逆文档频率(inverse document frequency, IDF)，用于计算该词区分文档的能力。

$$
idf=log(\frac{文本总数}{包含该单词的文本的数量+1})
$$

**Note**：这里的$idf$本质是信息熵，作为词的权重，与词频$tf$相乘构成了某词的$tfidf$值。$tfidf$值与一个词在文档中的出现次数成正比，与该词在整个语料中的出现次数成反比。某个词的TF-IDF值就越大,对文章的重要性越高。 



### 四、深度学习方法进行文本分类

#### 1、词向量

前面提到的方法都是用词在词表中的位置来表示一个词，而深度学习则将每个词表示成一个 n 维的稠密、连续的实数向量 （即词向量）。我们认为，通过训练和学习，词向量能蕴含部分词的语义信息，例如，意思相近的两个词，它们的词向量的相似度（比如余弦相似度、欧式距离的值）也比较高。

词是构成文本的基本元素，一段文本对应有一段**词向量序列**， 利用深度学习方法对文本进行表示，其实就是利用神经网络对词向量序列进行特征提取。即神经网络以词向量序列作为输入，经过网络的表示得到一个文本向量，最后在网络的输出层，对文本向量进行分类。

<div align="center"><img src="/img/1572067428348.png" alt="1572067428348" style="zoom:100%;" /></div>
词向量的三种**处理方式**：

- 模型训练前随机初始化，将词向量作为模型参数，随着训练任务的进行，和神经网络结构中的参数一起学习和更新。
- 直接使用Word2vec、Glove、FastText、ELMo、Bert等预训练方法的预训练的词向量，这些词向量不会随着文本分类任务的训练进行更新。
- 使用预训练的词向量，并使其随着任务的训练进行更新。

#### 2、基于RNN的文本分类模型

##### (1) 什么是RNN

 循环神经网络（Recurrent Neural Network, RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接递归神经网络（recursive neural network）。 

- RNN网络结构

  <div align="center"><img src="/img/rnn.png" alt="rnn" style="zoom:60%;"/></div> 

- RNN对应的数学计算
  $$
  h_t = \sigma(w_{1} x_t + b_{1}  +  w_{2} h_{t-1} + b_{2})
  $$
  $其中$

  $x_t \in R^{e×1}，h_{t-1} \in R^{d×1}$

  $x_t为第t个单词w_t对应的词向量，h_{t-1}为前一个词向量通过RNN网络的输出(称为隐状态)。$

  $w_1 \in R^{d×e}，b_1 \in R^{d×1}，w_2 \in R^{d×d}，b_2 \in R^{d×1}$

  $w_1,w_2,b_1,b_2为RNN网络的参数，随着任务训练进行更新。$

  $\sigma()为激活函数，可以是sigmoid()、tanh()、relu()等，用于增加网络的非线性。$

##### (2) 使用RNN进行文本分类（TextRNN）

第一步：将文本词向量序列$X=[x_1,x_2,...,x_n],\ $$x_t \in R^{e×1}$依次输入RNN网络，得到隐状态序列$H=[h_1,h_2,...,h_n],\ $$h_t \in R^{d×1}$。

我们可以通过以下三种方式获得文本表示：

- 文本表示1：将最后一个单词对应的隐状态$h_n$作为文本的向量表示。

- 文本表示2：将每个单词对应的隐状态$h_t$加和求平均作为文本的向量表示。

- 文本表示3：将每个单词对应的隐状态$h_t$求加权平均作为文本的向量表示。$（TextRNN + Attention机制）$

第二步：根据分类任务，选择带sigmoid或者softmax函数的输出层，对得到的文本向量进行分类。

$以第二种文本表示为例的分类过程如下：$

<div align="center"><img src="/img/1572074356869.png" alt="1572074356869" style="zoom:50%;" /></div>


#### 3、基于CNN的文本分类模型

##### (1) 什么是CNN

 卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络。卷积神经网络核心结构为卷积层和池化层。

- 卷积层（特征提取）

  让一定尺寸的**卷积核**在图像（某维度的矩阵）上，以一定的步长进行滑动，卷积核每到达一个位置，都做一次卷积运算，通过此方式能够得到一个特征图（矩阵或向量）。

- 池化层（ 特征选择和信息过滤 ）

  让一定尺寸的**池化窗口**在图像（某维度的矩阵）上，以一定的步长进行滑动，池化窗口每到达一个位置，都做一次池化运算，通过此方式能够得到一个降采样特征图（矩阵或向量）。

  池化运算：可以是max-pooling，mean-pooling等。

**Note**：卷积核是卷积神经网络需要随着模型训练进行更新的参数。卷积核、池化窗口尺寸、数量、步长都是可以认为设定的超参数。

- 卷积、池化操作示例

  <div align="center"><img src="/img/conv-1572080355818.jpg" alt="conv" style="zoom:67%;" /></div>

##### (2)使用CNN进行文本分类（TextCNN）

第一步：将文本的词向量序列排列成词向量矩阵，使用不同尺寸的卷积核和池化窗口对该矩阵进行卷积运算和池化操作进行特征提取，再将所提取的特征进行拼接作为文本的向量表示。

第二步：根据分类任务，选择带sigmoid或者softmax函数的输出层，对得到的文本向量进行分类。

示例：

<div alingn="center"><img src="/img/text_convolution.png" alt="img" style="zoom:80%;" /></div>
  上图简化版：

<div align="center"><img src="/img/1572081218727.png" alt="1572081218727" style="zoom:90%;" /></div>
#### 4、补充

- 其他的文本分类模型有TextRCNN，DPCNN、HAN、Transformer等等，这些模型与前面提到的深度学习方法的不同都只在于文本表示部分。
- 文本分类性能提升方法：预训练模型和方法，如Elmo、GPT、Bert、XLNet、RoBERTa等等。











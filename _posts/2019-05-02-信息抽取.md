---
layout:     post
title:      Information Extraction(IE)
subtitle:   命名实体识别和关系抽取
date:       2019-05-02
author:     biggan
header-img: img/post-bg-swift.jpg
catalog: true
tags:
    - NER
    - RE
---

## 信息抽取

从大规模非结构或半结构的自然语言文本中抽取结构化信息。主要任务有：

- 实体抽取、命名实体识别（Named Entity Recognition，NER）
- 关系抽取（Relation Extraction，RE）
- 事件抽取



#### 一、命名实体识别（NER）

##### 1.NER概述

- 指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。 

  一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。当然也可以定义其他的类别。

- 通常包括两部分：

  (1)实体边界识别；

  (2)确定实体类别（人名、地名、机构名或其他）。

- NER应用

  NER是NLP中一项基础性关键任务，是关系抽取、事件抽取、知识图谱、机器翻译、问答系统等诸多NLP任务的基础。例如：

  机器翻译中，命名实体的翻译常会有一些特殊翻译规则，准确识别出文本中的命名实体，对提高机器翻译的效果有重要的意义。

  信息检索中，命名实体可以用来提高和改进检索系统的效果，当用户输入“重大”时，可以发现用户更想检索的是“重庆大学”，而不是其对应的形容词含义。

- 中文与英文NER相比

  英语中的命名实体具有比较明显的形式标志，即实体中的每个词的第一个字母要大写，所以实体边界识别相对容易，任务的重点是确定实体的类别。

  中文命名实体识别任务和英语相比，更加复杂，而且相对于实体类别标注子任务，实体边界的识别更加困难。**难点**主要存在于：（1）汉语文本没有类似英文文本中空格之类的显式标示词的边界标示符，命名实体识别的第一步就是确定词的边界，即分词；（2）汉语分词和命名实体识别互相影响；（3）除了英语中定义的实体，外国人名译名和地名译名是存在于汉语中的两类特殊实体类型；（4）现代汉语文本，尤其是网络汉语文本，常出现中英文交替使用，这时汉语命名实体识别的任务还包括识别其中的英文命名实体；（5）命名实体存在嵌套现象，如“北京大学第三医院”这一组织机构名中还嵌套着同样可以作为组织机构名的“北京大学”，而且这种现象在组织机构名中尤其严重。（6）中文里广泛存在简化表达现象，如“北医三院”、“国科大”，乃至简化表达构成的命名实体，如“国科大桥”。（7）中文的用字灵活多变，有些词语在脱离上下文语境的情况下无法判断是否是命名实体，而且就算是命名实体，当其处在不同的上下文语境下也可能是不同的实体类型。
  
- NER存在的问题

  1. 实体数量巨大，而且在不断增长。
  2. 实体组合多。很多实体都是组合词，包括中英文组合也很多。
  3. 歧义多。比如“李飞飞起来了”，可以识别出"李飞"或"李飞飞"

##### 2.NER的三种方法

###### （1）基于规则的方法

&emsp;利用手工编写的规则，将文本与规则进行匹配来识别出命名实体。例如：“大学”、“医院”等词语可作为组织机构名的结尾。还可以利用到词性、句法信息。在构建规则的过程中往往需要大量的语言学知识，不同语言的识别规则不尽相同，而且需要谨慎处理规则之间的冲突问题；此外，构建规则的过程费时费力、可移植性不好。

###### （2）基于特征模板的方法

&emsp;统计机器学习方法将 NER 视作序列标注任务，利用大规模语料来学习出标注模型，从而对句子的各个位置进行标注。

&emsp; NER 任务中常用的模型包括生成式模型HMM、判别式模型CRF等。比较流行的方法是**特征模板 + CRF**的方案：特征模板通常是人工定义的一些二值特征函数，试图挖掘命名实体内部以及上下文的构成特点。对于句子中的给定位置来说，提特征的位置是一个窗口，即上下文位置。而且，不同的特征模板之间可以进行组合来形成一个新的特征模板。**CRF的优点**在于其为一个位置进行标注的过程中可以利用到此前已经标注的信息，利用Viterbi解码来得到最优序列。对句子中的各个位置提取特征时，满足条件的特征取值为1，不满足条件的特征取值为0；然后把特征喂给CRF，training阶段建模标签的转移，进而在inference阶段为测试句子的各个位置做标注。

###### （3）基于深度学习的方法

&emsp;深度学习方法也是将 NER 视作序列标注任务。

- 常见序列标注方式

  1. BIO标注集

     B表示实体的首字，I表示实体的非首字，O代表该字不属于命名实体的一部分。

     比如，某token被标注为B-P，B代表实体首字，P代表实体类型，即这是一个P类型实体的首字。

  2. BIOSE标注集
  
     B，即Begin，表示实体首字；
  
     I，即Intermediate，表示实体中间的字；
  
     E，即End，表示实体尾字；
  
     S，即Single，表示单个字符构成实体；
  
     O，即Other，表示其他，用于标记非实体部分的无关字符。
  
     比如，某token被标注为I-P，I代表实体中间的字，P代表实体类型，即这是一个P类型实体的中间字。

- RNN+Softmax

&emsp;将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征，Softmax来预测每个token的标签。这种方法使得模型的训练成为一个端到端的整体过程，而非传统的pipeline，不依赖特征工程，是一种数据驱动的方法；但网络变种多、对参数设置依赖大，模型可解释性差。此外，这种方法的一个**缺点**是对每个token打标签的过程中是独立的分类，**不能直接利用上文已经预测的标签**（只能靠隐状态传递上文信息），进而导致预测出的标签序列可能是非法的，例如标签B-PER后面是不可能紧跟着I-LOC的，但Softmax不会利用到这个信息。

- LSTM+CRF

&emsp;LSTM-CRF 模型做序列标注。LSTM层后接入CRF层来做标签预测，使得**标注过程不再是对各个token独立分类**。如果仅仅利用Softmax进行标签的预测，每一步都是挑选一个最大概率值的label输出，没有考虑Label之间的限制关系，比如BIOSE标注方式中，B-P后不能跟B-P。

![20180528211145954](img/20180528211145954-1556774016289.png)

##### 3.基于字的BiLSTM-CRF模型

###### （1）模型结构

<img src="https://images2017.cnblogs.com/blog/1008922/201707/1008922-20170726163008156-1916269133.png" width="70%"/>

###### （2）结构解析

1. $ look-up $层

   &emsp;利用预训练或随机初始化的**$embedding$矩阵**将句子中的每个字 $x_i$ 由one-hot向量映射为低维稠密的字向量（character embedding）$xi∈R^d$ ，$d$ 是$embedding$的维度。

2. $BiLSTM$层

   &emsp;将一个句子的char embedding序列 $(x_1,x_2,...,x_n)$作为双向LSTM各个时间步的输入，再将正向LSTM输出的隐状态序列$ (\vec{h_1}, \vec{h_2} ,..., \vec{h_n}) $与反向LSTM的$ (\overleftarrow{h_1}, \overleftarrow{h_2} ,..., \overleftarrow{h_n}) $在各个位置输出的隐状态进行按位置拼接 $ht=[\overleftarrow{h_t};\vec {h_t}]∈R_m$，得到完整的隐状态序列

   ​													$$(h1,h2,...,hn)∈R^{n×m}$$

3. CRF层

   - 先接入一个线性层，将隐状态向量从 $m$ 维映射到$ k $维，$k$ 是标注集的标签数，从而得到句子特征，记作矩阵 $P=(p_1,p_2,...,p_n)∈R^{n×k} $。

   - 再接入CRF层，进行句子级的序列标注。

     1. CRF层的参数是一个 $(k+2)×(k+2)$的矩阵 $A$ ，$A_{ij}$表示的是从第 $i$个标签到第$ j$个标签的转移得分。之所以要加2是因为要为句子首部添加一个起始状态以及为句子尾部添加一个终止状态。

     2. 记一个长度等于句子长度的标签序列 $y=(y_1,y_2,...,y_n)$ ，那么模型对于句子 $x$的标签等于 $y$ 的打分为

        ![TIM截图20190502105307](img/TIM截图20190502105307.jpg)
        
        $P_{i,y_i}$为第i个位置输出为$y_i$的概率，$A_{y_i,y_{i+1}}$为从$y_i$到$y_{i+1}$的转移概率。
     
   - 最后接入Softmax层，得到归一化的概率。
   
     ![TIM截图20190502104725](img/TIM截图20190502104725-1556765873443.jpg)
     
     $y’ \in Y,Y$为句子所有可能标注序列集合。
   
###### （3）模型训练

1. 训练会更新的参数

   BiLSTM、Linear、CRF的转移矩阵概率A

2. 目标函数

    模型训练时通过最大化对数似然函数，对真实标记序列y的概率取$P(y|x)$取$log$：

      ![TIM截图20190502110520](img/TIM%E6%88%AA%E5%9B%BE20190502110520-1556773287682.jpg)

   损失函数对目标函数取负即可。

###### （4）模型预测

   模型在预测过程（解码）时使用动态规划的Viterbi算法来求解最优路径：

   ![TIM截图20190502110227](img/TIM截图20190502110227.jpg)
#### 二、关系抽取（RE）

##### 1.什么是关系抽取

&emsp;从文本中识别实体并抽取实体之间的语义关系，即给定关系集合$R$，从文本中抽取出满足关系集合R中某一关系的三元组（triples）。

&emsp;三元组：由一个实体对$<e1, e2>$和实体之间关系$r$组成，可表示为$r(e1,e2)$或者$(e1, r, e2)$，其中的关系$r$属于给定关系集合$R$。

例如：从下面这段文本中抽取出如下三元组：

> *International Business Machines Corporation (IBM or the company) was incorporated in the State of New York on June 16, 1911.*
>
> - Founding-year (IBM, 1911)
> - Founding-location (IBM, New York)

##### 2.关系抽取的应用

- 创建新的结构化知识库(knowledge base)并且增强现有知识库
- 构建垂直领域知识图谱：医疗，化工，农业，教育等
- 支持上层应用：问答，搜索，推理等。

##### 3.关系抽取的方法

###### （1）基于规则方法

1.完全基于规则

&emsp;编写句型和关系的对应规则，即满足特定形式的句型即可抽取出相应的关系。

> Rule：A located in B  $\longrightarrow\longrightarrow$ located in (A,  B)
>
> Text：Alibaba located in Hanzhou
>
> triples：located in (Alibaba,  Hanzhou)

2.规则+命名实体识别

&emsp;基于关系一般发生在特定实体之间的思想，对规则加上实体类别的限定。

> Rule：A (Organization) located in B (Loaction)   $\longrightarrow\longrightarrow$ located in (A,  B)
>
> Text：Alibaba located in Hanzhou
>
> triples：located in (Alibaba,  Hanzhou)

3.基于规则方法的优缺点

&emsp;人工规则精度高；但是人工成本高、召回率低。

###### （2）有监督的学习方法

&emsp;将关系抽取任务当作分类问题，根据训练数据设计有效的特征，从而学习各种分类模型，然后使用训练好的分类器预测关系。

1.流水线的方法（Pipelined Method）

- 思想

&emsp;输入一个句子，首先进行命名实体识别，然后对识别出来的实体进行两两组合，再进行关系分类，最后把存在实体关系的三元组作为输入。

- 缺点

&emsp;1）错误传播，实体识别模块的错误会影响到下面的关系分类性能；

&emsp;2）忽视了两个子任务之间存在的关系；

&emsp;3）产生了没必要的冗余信息，有关系的实体会带来多余信息，提升错误率。

2.实体识别和关系抽取联合学习（Joint Learning）

- 思想

&emsp;输入一个句子，通过实体识别和关系抽取联合模型，直接得到有关系的实体三元组。

- 基于序列标注策略

  > 2017年ACL论文《Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme》

  对句子中每一个单词按如下方式进行标注：

  &emsp;1）实体中词的位置信息{B（实体开始），I（实体内部），E（实体结尾），S（单个实体）}；

  &emsp;2）关系类型信息{根据预先定义的关系类型进行编码}；

  &emsp;3）实体角色信息{1（实体1），2（实体2）}。

  ![img](img/670089-20171022170357131-1667566271.png)

  &emsp;根据标签序列，将同样关系类型的实体合并成一个三元组作为最后的结果，如果一个句子包含一个以上同一类型的关系，那么就采用就近原则来进行配对。

  **目标函数**：

  ![img](img/670089-20171022170357646-737214308.png)

  **问题：**

  一个关系，对应有多个主体，多个实体，主体和实体怎么配对？

3.有监督方法的评估

&emsp;计算precision，recall以及F1。

4.有监督方法的问题

&emsp;该方法的问题在于需要大量的人工标注训练语料，而语料标注工作通常非常耗时耗力。

###### （3）半监督的学习方法

&emsp;主要采用Bootstrapping进行关系抽取。对于要抽取的关系，该方法首先手工设定若干种子实例，然后迭代地从数据中抽取关系对应的关系模板和更多的实例。

&emsp;具体步骤：

- 收集一些关系为R的种子对（seed pair）

- 迭代

  

  - 找到包含这些单词对的句子
  - 找到这些单词对的上下文，泛化成模式（pattern）
  - 找到新的单词对

###### （4）无监督的学习方法

&emsp;基于假设：拥有相同语义关系的实体对拥有相似的上下文信息。

&emsp;利用每个实体对所对应的上下文信息来代表该实体对的语义关系，并对所有实体对的语义关系进行聚类。

&emsp;具体步骤：

- 对每个关系（relation）中的每一对元组，在语料库中找到同时包含这两个实体的句子
- 抽取高频的特征（语法分析、单词等）
- 用这些模式来训练监督模型



**参考链接**

命名实体识别：

https://www.cnblogs.com/Determined22/p/7238342.html

https://blog.csdn.net/bobobe/article/details/80489303

关系抽取：

https://blog.csdn.net/qq_27009517/article/details/80065789

https://blog.csdn.net/sinat_36972314/article/details/80266698
